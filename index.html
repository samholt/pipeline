
<!doctype html>

<title>Experiments in Handwriting with a Neural Network — Distill</title>

<body>

<div class="post">
  <div class="intro w-page">
    <h1>Four Experiments in Handwriting with a Neural Network</h1>
    <h2>We’ll start with a fun one that tries to predict your strokes as you write</h2>
  </div>
  <p> Neural networks are an extremely successful approach to machine learning, but it’s tricky to understand why they behave the way they do. This has sparked a lot of interest and effort around trying to understand and visualize them, which we think is so far just scratching the surface of what is possible.</p>

  <p>In this article we will try to push forward in this direction by taking a generative model of handwriting<sup><a href="#footnote2">2</a></sup> and visualizing it in a number of ways. In the end we don’t have some ultimate answer or visualization, but we do have some interesting ideas to share. Ultimately we hope they make it easier to divine some meaning from the internals of these model.</p>

  <h3>Looking at the Output of the Model</h3>

  <p>Our first experiment is the most obvious: when we want to see how well someone has learned a task we usually ask them to demonstrate it. So, let’s ask our model to write something for us and see how well it does.</p>

  <p>Most of the marks are gibberish, but many of them are surprisingly convincing. Some real (or real-ish) words even start to appear. One surprising thing you’ll notice is that the general style of handwriting is more or less consistent within a sample. This is because the type of architecture used for this model (LSTM) has a mechanism for remembering previous strokes. It is therefore able to remember things like how loopy or jerky, or which letter preceeded the current one. (For more on LSTMs and how they can remember, Chris Olah has a <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">good primer</a>.)</p>
</body>
